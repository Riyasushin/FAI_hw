# HW01



## 1、请简述什么是贝叶斯定理，什么是最大似然估计(MLE)，什么是最大后验估计(MAP)。

##### 贝叶斯定理

贝叶斯学派认为概率是对结果的信念程度

公式是

```math
P(A | B) = \frac{P(B|A)P(A)}{P(B)} \\
```

其中：

- P(A|B) 是后验概率，观察到出现的现象后，对当前情况下的概率大更新
- P(B|A) 是似然概率，不同参数下y的可观测数据集中的可能性
- P(A)是先验概率，在观察前对概率的猜测
- P(B) 是归一化常数，让总和为1



##### MLE

- 认为参数 $\theta$ 为一个常数，目的是找的这个结果$\theta$ s.t.:

  $$\theta_{MLE} = arg \underset{\theta}{max} P (B | \theta)$$

- 对于这个概率模型的参数 $\theta$ ，求最好的 $\theta$ 让 出现得到的数据的概率最大

```math
方便些取ln:lnP(X|\theta) = \sum lnP(x_i|\theta) \\
找这样的参数 \theta 让 P(X|\theta)最大 （这里的\theta 代所有相关的参数）
```

- 选择一个模型来适配数据时，实际上是在做出一个关于数据可能的底层过程的假设

- 假设数据生成过程是由某个分布控制的，不是选择某个参数的概率分布

##### MAP

- 认为参数 $\theta$也是随机变量，假设有先验分布

- 用一个设好的参数的贝塔分布作为 $\theta$ 的先验分布 $p(\theta)$ （对参数进行了分布建模）

  $$\theta_{MAP} = arg \underset{\theta}{max} P(B|\theta) P (\theta)$$
  
  
  



---

## 2、设𝑋~𝑁($𝜇$, $𝜎^2$), $𝜇$, $𝜎^2$为未知参数，$x_1, 𝑥_2, \dots, 𝑥_n$是来自𝑋的样本值，求𝜇, 𝜎!的最大似然估计量。 

```math

\begin{array}{l}
F(\mu, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x - \mu)^2}{2\sigma^2}} \\
\ln F = -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln{\sigma^2} - \frac{1}{2\sigma^2} \sum_{i=1}^n(x_i-\mu)^2 \\
\text{对 }\mu\text{ 求偏导：}\\
\frac{1}{\sigma^2}\sum(x_i - \mu) = 0 \\
\text{对 }\sigma^2\text{ 求偏导：} \\
-\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum_{i=1}^n(x_i-\mu)^2 = 0\\
\textbf{于是得到结论:}\\


\end{array}

```

```math
\mu = \frac{\sum_{i=1}^n x_i}{n} \\
\sigma = \frac{\sum_{i=1}^n x_i^2 - n \times \mu^2}{n}\\
```



## 3、请简述分类问题与回归问题的主要区别。

|          | 分类问题                          | 回归问题             |
| -------- | --------------------------------- | -------------------- |
| 数据结果 | 离散，比如one-hot Vector，        | 分布在连续的实数空间 |
| 常用loss | CrossEntropy等                    | 方差、均方差等       |
|          | 先通过softmax，得到最大概率为预测 | 连接一个FC作为输出层 |



## 4、请简述有监督学习与无监督学习的主要区别。

|            | 有监督学习                | 无监督学习                        |
| ---------- | ------------------------- | --------------------------------- |
| 数据集内容 | 数据都有label             | 数据没有label                     |
| 典型任务   | 分类、回归                | 聚类、降维、密度估计              |
| 结果预期   | 拟合得到一个$f(x)$近似$y$ | 希望机器自己学会某些隐含的pattern |

## 5、

> 给定数据 $D=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$, 用一个线性模型估计最接近真实 $\gamma_i$ (ground truth) 的连续标量 $Y$, $f(x_i)=w^Tx_i+b$, 使得 $f(x_i)\approx y_i$.
>
> 求最优 ($w^{*},b^{*}$) 使得 $f(x_i)$ 与 $y_i$ 之间的均方误差最小：
>
> $$
> (w^*,b^*)=\arg\min_{(w,b)}\sum_{i=1}^n(f(x_i)-y_i)^2
> $$
>
> 并解释 $(w^*,b^*)$ 何时有 closed form 解，何时没有 closed form 解。



最小二乘法：

```math
(w^*, b^*) = arg \space min \sum_{i=1}^n (y_i - wx_i + b)^2\\

```



```math
\beta = 
\begin{bmatrix}
w_1 \\
w_2 \\
\vdots\\
w_{p-1} \\
b \\

\end{bmatrix}
```

每个数据一行，每个标签一行，拿$x_i^{(p)} \equiv 1 $ 

```math
\textbf{A} =  
\begin{bmatrix}
X_1 \\
X_2 \\
\vdots \\
X_n
\end{bmatrix}
=
\begin{bmatrix}
x_1^{(1)} & \dots & x_1^{(p)} \\
\vdots & \ddots & \vdots \\
x_n^{(1)} & \dots & x_n^{(p)}\\
\end{bmatrix}
```

```math
\textbf{Y} =  
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
```

找 $\beta$ 使得: $$\hat{\beta} = arg \space \underset{\beta}{min} \frac{1}{n} \sum (X_i \beta - Y_i)^2 = arg \space \underset{\beta}{min} \frac{1}{n} (\textbf{A}\beta - \textbf{Y}^T(\textbf{A}\beta - \textbf{Y}) $$

n是定值，忽略

```math
\begin{aligned}
J(\beta) &= (\textbf{A}\beta - \textbf{Y}^T(\textbf{A}\beta - \textbf{Y})\\
&=\beta^{\textbf{T}} \textbf{A}^{T} \textbf{A} \beta-2 \beta^{\textbf{T}} \textbf{A}^{T} \textbf{Y}+\textbf{Y}^{T} \textbf{Y}\\

\end{aligned}
```

求偏导得出：

```math
2\textbf{A}^T\textbf{A}\beta - 2\textbf{A}^T\textbf{Y} = 0 \\
\therefore \hat{\beta} = (\textbf{A}^T\textbf{A})^{-1}\textbf{A}^T\textbf{Y} , 若 A 可逆
```

所以：

- A不可逆时无 closed form解
- A可逆时    有 closed form解







## 6、Ridge regression 问题的解具有什么特点，为什么？Lasso 问题的解具有什么特点？为什么？

#### Ridge regression:

- $L1 = reg \times \sum| \beta_j | $ 
- 对于所有系数的惩罚程度相同，导致一些系数直接为0
- 产生稀疏解

#### Lasso：

- $$L2 =$$1²·L² reg \times \sum \beta^2$$
- 对绝对值大的$\beta$惩罚力度更大
- 系数平滑地趋近于0，有些$w$更小





## 7、请从 model function、loss function、optimization solution 三个方面比较 Linear regression与 Logistic regression 的异同。

|                       | Linear regression                        | Logistic regression                  |
| --------------------- | ---------------------------------------- | ------------------------------------ |
| model function        | $$f(x) = W^T x + b$$                     | $$f(x)=\frac{1}{1 + e^{-W^Tx + b}}$$ |
| loss function         | MSE，衡量$\hat{y}$和$y$的差异            | CrossEntropy，衡量分布的差异         |
| optimization solution | 最小二乘法直接算参数<br />梯度下降近似解 | 一般只能梯度下降近似解               |



## 8、K-近邻分类器的超参数是什么？怎么选择 K-近邻分类器的超参数？

#### 超参

- $k$：邻居个数
- 距离度量方式：欧氏距离、曼哈顿距离

#### 怎么选择

交叉验证法

- 把数据集分为n（给定）个部分，一部分用于训练（$70\% \sim 80\%$），另一部分用于验证($20\% \sim 30\%$)
- 选择验证集accuracy最大的那个$k$.
